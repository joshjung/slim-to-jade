{
  "author": {
    "name": "Pierre Curto"
  },
  "name": "atok",
  "description": "Fast, easy and dynamic tokenizer for Node Streams",
  "keywords": [
    "tokenizer",
    "token",
    "async",
    "stream"
  ],
  "version": "0.4.3",
  "homepage": "http://github.com/pierrec/node-atok",
  "repository": {
    "type": "git",
    "url": "git://github.com/pierrec/node-atok.git"
  },
  "main": "./lib/tokenizer.js",
  "bugs": {
    "url": "http://github.com/pierrec/node-atok/issues"
  },
  "licenses": [
    {
      "type": "MIT",
      "url": "http://github.com/pierrec/node-atok/raw/master/LICENSE"
    }
  ],
  "engines": {
    "node": ">= 0.6"
  },
  "dependencies": {
    "inherits": ">= 1.0",
    "ev": ">= 0.0.3",
    "fnutils": ">= 0.0.1",
    "buffertools": ">= 1.1"
  },
  "devDependencies": {
    "mocha": ">= 0.10",
    "bench": ">= 0.3",
    "ekam": ">= 0.0.7",
    "dox": "latest",
    "dox-template": "latest",
    "fstream": "latest"
  },
  "scripts": {
    "test": "mocha test/*-test.js"
  },
  "readme": "# ATOK - async tokenizer\n\n\n## Overview\n\nAtok is a fast, easy and dynamic tokenizer designed for use with [node.js](http://nodejs.org). It is based around the [Stream](http://nodejs.org/docs/latest/api/streams.html) concept and is implemented as a read/write one.\n\nIt was originally inspired by [node-tokenizer](https://github.com/floby/node-tokenizer), but quickly grew into its own form as I wanted it to be RegExp agnostic so it could be used on node Buffer intances and more importantly *faster*.\n\nAtok is built using [ekam](https://github.com/pierrec/node-ekam) as it abuses includes and dynamic method generation.\n\nAtok is the fundation for the [atok-parser](https://github.com/pierrec/node-atok-parser), which provides the environment for quickly building efficient and easier to maintain parsers.\n\n\n## Core concepts\n\nFirst let's see some definitions. In atok's terms:\n\n* a `subrule` is an atomic check against the current data. It can be represented by a user defined function (rarely), a string or a number, or an array of those, as well as specific objects defining a range of values for instance (e.g. { start: 'a', end: 'z' } is equivalent to /[a-z]/ in RegExp)\n* a `rule` is an __ordered__ combination of subrules. Each subrule is evaluated in order and if any fails, the whole rule is considered failed. If all of them are valid, then the handler supplied at rule instanciation is triggered, or if none was supplied, a data event is emitted instead.\n* a `ruleSet` is a list of `rules` that are saved under a given name. Using `ruleSets` is useful when writting a parser to break down its complexity into smaller, easier to solve chunks. RuleSets can be created or altered __on the fly__ by any of its handlers.\n* a `property` is an option applicable to the current rules being created.\n    * properties are set using their own methods. For instance, a `rule` may load a different `ruleSet` upon match using `next()`\n    * properties are defined before the rules they need to be applied to. E.g. atok.next('rules2').addRule(...)\n    * once defined, properties are applied to all subsequent rules, unless turned off by calling the property method with no argument or false. E.g. in atok.next('rules2').addRule(...).addRule(...).next().addRule(...) only rule 1 and 2 will load the `ruleSet` _rules2_ if they match.\n\n\nThe default workflow in atok is as follow:\n\n* data is provided to the tokenizer\n* the tokenizer evaluates each of its `rules` against it (its current `ruleSet`)\n    * if none match, it stops and waits for more\n    * if one matches, it triggers the handler/emit an event, then go back to rules evaluation\n\n\nThe default workflow can be altered using the `continue()` and `next()` property methods:\n\n* `continue(jump[, jumpOnFail])`: the next `rule` being checked is relative to the one that matched, downward if jump value is positive, upward if negative. In case the `rule` fails, by default, the tokenizer will process to the next one. This can be modified by specifying the jumpOnFail value.\n    * `continue(0)`: go to the next `rule` on success\n    * `continue(-1)`: reevaluate the current `rule` on success\n    * `continue(-2)`: go to the previous `rule` on success\n* `next(ruleSet[, index])`: when the handler returns, the tokenizer will evaluate rules from the new `ruleSet`, starting at the first one or the one at _index_.\n\n\nIt is important to note that the tokenizer is _highly_ dynamic:\n\n* `ruleSets` can be changed by handlers\n* `rules` and `ruleSets` can be created by handlers based on the data being processed\n* after a match, the tokenizer can branch to a different `ruleSet`\n\n\n## Download\n\nAtok is published on node package manager (npm). To install, do:\n\n    npm install atok\n\n\n## Quick example\n\nGiven the following json to be parsed:\n\n    [\"Hello world!\"]\n\nThe following code would be a very simple JSON parser for it.\n\n``` javascript\nvar Tokenizer = require('atok')\nvar tok = new Tokenizer\n\n// Define the parser rules\n// By default it will emit data events when a rule is matched\ntok\n    // Define the quiet property for the following rules (quiet=dont tokenize but emit/trigger the handler)\n    // Only used to improve performance\n    .quiet(true)\n        // first argument is a match on the current position in the buffer\n        .addRule('[', 'array-start')\n        .addRule(']', 'array-end')\n    .quiet() // Turn the quiet property off\n    // The second pattern will only match if it is not escaped (default escape character=\\)\n    .escaped(true)\n        .addRule('\"', '\"', 'string')\n    .escaped()\n    // Array item separator\n    .addRule(',', 'separator')\n    // Skip the match, in this case whitespaces\n    .ignore(true)\n        .addRule([' ','\\n', '\\t','\\r'], 'whitespaces')\n    .ignore()\n\n// Setup some variables\nvar stack = []\nvar inArray = false\n\n// Attach listeners to the tokenizer\ntok.on('data', function (token, idx, type) {\n    // token=the matched data\n    // idx=when using array of patterns, the index of the matched pattern\n    // type=string identifiers used in the rule definition\n    switch (type) {\n        case 'array-start':\n            stack.push([])\n            inArray = true\n        break\n        case 'array-end':\n            inArray = false\n        break\n        case 'string':\n            if (inArray)\n                stack[ stack.length-1 ].push(token)\n            else\n                throw new Error('only Arrays supported')\n        break\n        case 'separator':\n        break\n        default:\n            throw new Error('Unknown type: ' + type)\n    }\n})\ntok.on('end', function () {\n    console.log('results is of type', typeof stack[0], 'with', stack[0].length, 'item(s)')\n    console.log('results:', stack[0])\n    \n})\n\n// Send some data to be parsed!\ntok.end('[ \"Hello\", \"world!\" ]')\n```\n\n__Output__\n\n    results is object with 1 item(s)\n    results: [ 'Hello world!' ]\n\n\n## Documentation\n\nSee [here](http://pierrec.github.com/node-atok/).\n\n\n## Testing\n\nAtok has a fairly extended set of tests written for [mocha](https://github.com/visionmedia/mocha). See the [test](https://github.com/pierrec/node-atok/tree/master/test) directory.\n\n\n## Issues\n\nSee the TODO file.\n\n\n## License\n\nMIT [Here](https://github.com/pierrec/node-atok/tree/master/LICENSE)",
  "readmeFilename": "README.md",
  "_id": "atok@0.4.3",
  "dist": {
    "shasum": "a1670c0767c5af7b34ee0c0bd5e3b3fb8c15b554",
    "tarball": "http://registry.npmjs.org/atok/-/atok-0.4.3.tgz"
  },
  "_npmVersion": "1.1.66",
  "_npmUser": {
    "name": "pierrec",
    "email": "pierre.curto@gmail.com"
  },
  "maintainers": [
    {
      "name": "pierrec",
      "email": "pierre.curto@gmail.com"
    }
  ],
  "directories": {},
  "_shasum": "a1670c0767c5af7b34ee0c0bd5e3b3fb8c15b554",
  "_resolved": "https://registry.npmjs.org/atok/-/atok-0.4.3.tgz",
  "_from": "atok@*"
}
